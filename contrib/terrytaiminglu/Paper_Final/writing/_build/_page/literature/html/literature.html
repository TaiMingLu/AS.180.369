
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Abstract &#8212; Literature Review</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'literature';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="#">
  
  
  
  
  
  
    <p class="title logo__title">Literature Review</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Abstract
                </a>
            </li>
        </ul>
        
    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/literature.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Abstract</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Abstract</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#privacy-leakage-in-llms">Privacy Leakage in LLMs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#method">Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analytical-framework">Analytical Framework</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experimental-setup">Experimental Setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-preparation">Dataset Preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">Model Evaluation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis">Analysis</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-privacy-leakage-distribution">Data Privacy Leakage Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pii-type-distribution">1. PII Type Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pii-occurrence-rate-per-document">2. PII Occurrence Rate per Document</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-analysis-of-person-entities">3. Detailed Analysis of “PERSON” Entities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-ip-address-entities">4. Analysis of “IP_ADDRESS” Entities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-data-privacy-leakage-on-llms">Effect of Data Privacy Leakage on LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis-and-statistical-significance">1.Regression Analysis and Statistical Significance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implications-of-the-findings">2. Implications of the Findings</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">Bibliography</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <hr class="docutils" />
<p>title: Privacy Leakage in Large Language Models
date: 2024-10-07
authors:</p>
<ul class="simple">
<li><p>name: Terry Taiming Lu</p></li>
</ul>
<hr class="docutils" />
<section id="abstract">
<h1>Abstract<a class="headerlink" href="#abstract" title="Link to this heading">#</a></h1>
<p>Large language models (LLMs) trained on vast, often sensitive datasets can inadvertently memorize and expose private information. This study investigates how specific conditions—such as entity frequency and data repetition—contribute to privacy leakage when LLMs generate text. Using a representative web dataset, we identify the presence of personal names, geopolitical entities, and IP addresses that appear frequently and risk being replicated in model outputs. Our analysis demonstrates a positive, statistically significant correlation between the frequency of certain sensitive data in training and its likelihood of appearing in generated text. These findings underscore the need for more robust privacy-preserving strategies, including differential privacy and data anonymization techniques, to safeguard sensitive information. Our work lays a foundation for understanding the inherent vulnerabilities in LLMs and guides future research toward securing models against privacy breaches.</p>
</section>
<section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h1>
<p>Imagine a world where the very technology that powers our daily lives—answering our questions, assisting in our work, and even entertaining us—becomes a potential threat to our privacy. As large language models (LLMs) continue to evolve, trained on enormous amounts of data, including sensitive personal details, the risk of unintentional privacy leaks grows. Understanding these risks requires a closer examination of how LLMs operate and the potential vulnerabilities inherent in their design, particularly when handling sensitive information.</p>
<p>By processing vast amounts of sensitive data, ranging from personal consumer details to government-held records, LLMs and other deep learning systems can unintentionally expose confidential information, leading to significant financial and reputational harm. These privacy leaks pose not only a direct economic cost—such as regulatory fines, litigation expenses, and erosion of consumer trust—but also broader impacts on innovation and market efficiency.</p>
<p>In this paper, we investigate the mechanisms behind privacy leaks in LLMs. Specifically, we examine how training datasets, often containing personal identifiers and other sensitive information, contribute to memorization and subsequent leakage during inference. Using quantitative analysis, we explore the extent to which sensitive tokens—such as names, addresses, and network identifiers—are memorized, focusing on their frequency and distribution in the data . Our analysis demonstrates a positive correlation between data occurrence in the training corpus and its generation probability during inference. This suggests that the more frequently sensitive information appears in the training data, the higher the likelihood of it being memorized and subsequently leaked by the model.</p>
<p>By focusing on this correlation, we aim to provide a clear understanding of how leakage risks are amplified by certain training data characteristics. This work emphasizes the importance of careful dataset curation and privacy-preserving preprocessing techniques to minimize the potential for sensitive information exposure.</p>
<p>Our contribution is two-fold:</p>
<ol class="arabic simple">
<li><p>We provided quatitative analysis on privacy leakage in internal snapshots popular for training large language models.</p></li>
<li><p>We analyze the specific vulnerabilities within LLMs that lead to privacy leaks, providing a technical overview of how private information may be inadvertently exposed.</p></li>
</ol>
<p>|## Background</p>
<p><strong>Large Language Models.</strong> Large Language Models (LLMs) have emerged as transformative technologies in artificial intelligence, capable of performing various tasks such as natural language understanding, text generation, and translation <span id="id1">[<a class="reference internal" href="#id22" title="A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS). 2017.">Vaswani <em>et al.</em>, 2017</a>, <a class="reference internal" href="#id23" title="A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 2019.">Radford <em>et al.</em>, 2019</a>, <a class="reference internal" href="#id24" title="C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 2020.">Raffel <em>et al.</em>, 2020</a>]</span>. These models, such as GPT-3 <span id="id2">[<a class="reference internal" href="#id25" title="T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS). 2020.">Brown <em>et al.</em>, 2020</a>]</span> and BERT <span id="id3">[<a class="reference internal" href="#id26" title="J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). 2019.">Devlin <em>et al.</em>, 2019</a>]</span>, are trained on massive datasets from diverse sources, including books, articles, and websites, enabling them to generate coherent and contextually appropriate text. Their capabilities make LLMs valuable across domains such as customer service, content creation, research assistance, healthcare <span id="id4">[<a class="reference internal" href="#id27" title="A. Esteva, A. Robicquet, B. Ramsundar, V. Kuleshov, M. DePristo, K. Chou, C. Cui, G. Corrado, S. Thrun, and J. Dean. A guide to deep learning in healthcare. Nature Medicine, 2019.">Esteva <em>et al.</em>, 2019</a>]</span>, and legal document processing <span id="id5">[<a class="reference internal" href="#id28" title="M. J. Bommarito and D. M. Katz. A study of artificial intelligence in legal document analysis. Journal of Artificial Intelligence Research, 2018.">Bommarito and Katz, 2018</a>]</span>. However, LLMs also present significant privacy challenges. During training, these models can inadvertently memorize sensitive or personally identifiable information, potentially exposing it during inference <span id="id6">[<a class="reference internal" href="#id29" title="N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, A. Oprea, and C. Raffel. Extracting training data from large language models. In USENIX Security Symposium. 2021.">Carlini <em>et al.</em>, 2021</a>]</span>. Such privacy leaks have raised concerns about their use in real-world applications, where the risk of exposing confidential information could have serious legal and economic repercussions <span id="id7">[<a class="reference internal" href="#id30" title="R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In Proceedings of the 2017 IEEE Symposium on Security and Privacy. 2017.">Shokri <em>et al.</em>, 2017</a>, <a class="reference internal" href="#id31" title="B. Jayaraman and D. Evans. Evaluating differentially private machine learning in practice. In Proceedings of the 28th USENIX Security Symposium. 2019.">Jayaraman and Evans, 2019</a>]</span>. Addressing these risks requires the development of privacy-preserving techniques such as differential privacy <span id="id8">[<a class="reference internal" href="#id32" title="M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 2016.">Abadi <em>et al.</em>, 2016</a>]</span> and data anonymization, as well as robust regulatory frameworks to protect data while fostering innovation <span id="id9">[<a class="reference internal" href="#id33" title="M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfinkel, A. Dafoe, P. Scharre, T. Zeitzoff, B. Filar, H. Anderson, H. Roff, R. Crootof, O. Evans, M. Page, J. Bryson, R. Yampolskiy, and D. Amodei. The malicious use of artificial intelligence: forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228, 2018.">Brundage <em>et al.</em>, 2018</a>]</span>.</p>
<p><strong>Data Privacy and Utility in AI Models.</strong> The balance between data privacy and utility is a crucial issue, particularly in the context of large-scale AI models. Differential privacy has emerged as a popular solution to protect sensitive information in datasets, but it often introduces significant noise, leading to reduced data accuracy and economic inefficiencies <span id="id10">[<a class="reference internal" href="#id17" title="S. Ruggles. When privacy protection goes wrong: how and why the 2020 census confidentiality program failed. Journal of Economic Perspectives, 2024. URL: https://doi.org/10.1257/JEP.38.2.201, doi:10.1257/JEP.38.2.201.">Ruggles, 2024</a>]</span>. This trade-off has been further examined in the context of health disparities, where privacy measures disproportionately distort data for smaller populations, raising concerns about fairness and resource allocation <span id="id11">[<a class="reference internal" href="#id18" title="A. R. Santos-Lozada, J. T. Howard, and A. M. Verdery. How differential privacy will affect our understanding of health disparities in the united states. Proceedings of the National Academy of Sciences, 2020. URL: https://doi.org/10.1073/PNAS.2003714117, doi:10.1073/PNAS.2003714117.">Santos-Lozada <em>et al.</em>, 2020</a>]</span>.
Traditional statistical disclosure methods have been defended as viable alternatives, suggesting that newer techniques like differential privacy may not always offer superior protection without substantial economic costs <span id="id12">[<a class="reference internal" href="#id19" title="K. Muralidhar and J. Domingo-Ferrer. A rejoinder to garfinkel (2023) – legacy statistical disclosure limitation techniques for protecting 2020 decennial us census: still a viable option. Journal of Official Statistics, 2023. URL: https://doi.org/10.2478/JOS-2023-0019, doi:10.2478/JOS-2023-0019.">Muralidhar and Domingo-Ferrer, 2023</a>]</span>. In response, optimization frameworks have been proposed to find a middle ground, allowing for both privacy and data utility, though they require careful balancing to avoid significant losses in either area <span id="id13">[<a class="reference internal" href="#id20" title="V. J. Hotz, C. Bollinger, T. Komarova, C. Manski, R. Moffitt, D. Nekipelov, A. J. Sojourner, and B. Spencer. Balancing data privacy and usability in the federal statistical system. Proceedings of the National Academy of Sciences, 2022. URL: https://doi.org/10.1073/PNAS.2104906119, doi:10.1073/PNAS.2104906119.">Hotz <em>et al.</em>, 2022</a>]</span>.
The risks associated with privacy leakage from AI models, particularly in high-stakes sectors like healthcare and finance, underscore the need for better privacy-preserving techniques. Misuse of privacy mechanisms can lead to economic losses through reduced data reliability and non-compliance with regulations, making this a critical area for future research <span id="id14">[<a class="reference internal" href="#id21" title="J. Domingo-Ferrer, D. Sánchez, and A. Blanco-Justicia. The limits of differential privacy (and its misuse in data release and machine learning). Communications of the ACM, 2021. URL: https://doi.org/10.1145/3433638, doi:10.1145/3433638.">Domingo-Ferrer <em>et al.</em>, 2021</a>]</span>.</p>
</section>
<section id="privacy-leakage-in-llms">
<h1>Privacy Leakage in LLMs<a class="headerlink" href="#privacy-leakage-in-llms" title="Link to this heading">#</a></h1>
<section id="problem-formulation">
<h2>Problem Formulation<a class="headerlink" href="#problem-formulation" title="Link to this heading">#</a></h2>
<p>The primary objective of this study is to investigate the potential privacy risks associated with large language models (LLMs). Specifically, we aim to understand how and under what conditions LLMs memorize sensitive information from their training data and how likely it is that such information can be exposed during inference. We focus on answering the following key questions:</p>
<ol class="arabic simple">
<li><p><strong>To what extent do LLMs memorize sensitive information during training?</strong></p></li>
<li><p><strong>What factors influence the likelihood of privacy leakage in LLMs?</strong></p></li>
</ol>
<p>The goal is to quantify the trade-off between model utility and privacy risk, providing insight into how to train LLMs while minimizing the potential for privacy breaches.</p>
</section>
<section id="method">
<h2>Method<a class="headerlink" href="#method" title="Link to this heading">#</a></h2>
<p>The methodology focuses on analyzing privacy leakage risks in large language models (LLMs) by identifying correlations between the occurrence of sensitive information in training data and its likelihood of being generated during inference. The approach involves defining sensitive information patterns, evaluating token generation probabilities, and employing quantitative metrics to assess privacy risks.</p>
<section id="analytical-framework">
<h3>Analytical Framework<a class="headerlink" href="#analytical-framework" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>Token Generation Analysis</strong></p>
<ul class="simple">
<li><p>Identify sequences <span class="math notranslate nohighlight">\(S = \{t_1, t_2, \ldots, t_n\}\)</span> in the training data containing privacy-sensitive tokens.</p></li>
<li><p>Calculate the conditional probability <span class="math notranslate nohighlight">\(P(t_i | t_1, \ldots, t_{i-1})\)</span> for each token <span class="math notranslate nohighlight">\(t_i\)</span> in the sequence to evaluate the likelihood of generating the next token.</p></li>
<li><p>Aggregate these probabilities over the identified sequences to compute the overall likelihood of generating sensitive information:</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(
P(\text{sensitive} | \text{context}) = \prod_{i} P(t_i | t_1, \ldots, t_{i-1}),
\)</span></p>
<p>where <span class="math notranslate nohighlight">\(t_i\)</span> belongs to the sensitive sequence.</p>
</li>
<li><p><strong>Correlation Analysis</strong></p>
<ul class="simple">
<li><p>Measure the frequency of sensitive data occurrences in the training dataset.</p></li>
<li><p>Quantify the relationship between this frequency and the likelihood of such data being generated during inference. This analysis identifies trends linking higher data occurrence to increased generation probabilities.</p></li>
</ul>
</li>
<li><p><strong>Quantitative Metrics</strong></p>
<ul class="simple">
<li><p><strong>Token Generation Probability (TGP):</strong> Measures the likelihood of generating tokens associated with sensitive information during inference.</p></li>
<li><p><strong>Perplexity:</strong> Evaluates how well the model predicts privacy-sensitive sequences, with lower perplexity indicating higher memorization.</p></li>
<li><p><strong>Frequency Analysis:</strong> Tracks the distribution of sensitive information within the training dataset to identify high-risk categories.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="experimental-setup">
<h2>Experimental Setup<a class="headerlink" href="#experimental-setup" title="Link to this heading">#</a></h2>
<p>The experiments were conducted using the FineWeb dataset <span id="id15">[<a class="reference internal" href="#id35" title="Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: decanting the web for the finest text data at scale. 2024. URL: https://arxiv.org/abs/2406.17557, arXiv:2406.17557.">Penedo <em>et al.</em>, 2024</a>]</span>, a high-quality web-based corpus, alongside synthetic data containing intentionally generated sensitive information. The following steps outline the experimental setup:</p>
<section id="dataset-preparation">
<h3>Dataset Preparation<a class="headerlink" href="#dataset-preparation" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Data Selection</strong></p>
<ul class="simple">
<li><p>The FineWeb dataset was chosen for its diverse content and relevance to real-world applications. It includes web snapshots, organizational documents, and other text sources.</p></li>
<li><p>Synthetic data was generated to include randomized sensitive information, such as names, addresses, and network identifiers, to test the model’s handling of privacy-sensitive inputs.</p></li>
</ul>
</li>
<li><p><strong>Data Preprocessing</strong></p>
<ul class="simple">
<li><p><strong>Filtering Sensitive Content:</strong> Using regular expressions, documents containing PII (e.g., names, addresses, IP addresses) were extracted.</p></li>
<li><p><strong>Metadata Extraction:</strong> Timestamp and contextual metadata were included for additional analysis.</p></li>
<li><p><strong>Tokenization:</strong> Data was tokenized using GPT-4’s tokenizer to ensure compatibility with the model.</p></li>
</ul>
</li>
</ol>
</section>
<section id="model-evaluation">
<h3>Model Evaluation<a class="headerlink" href="#model-evaluation" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Token Generation Analysis</strong></p>
<ul class="simple">
<li><p>The model was prompted with contexts derived from the training data containing sensitive sequences.</p></li>
<li><p>Conditional probabilities <span class="math notranslate nohighlight">\(P(t_i | t_1, \ldots, t_{i-1})\)</span> were calculated for tokens within privacy-sensitive sequences.</p></li>
<li><p>The frequency of generating sensitive tokens in different contexts was recorded.</p></li>
</ul>
</li>
<li><p><strong>Quantitative Assessment</strong></p>
<ul class="simple">
<li><p>Metrics such as Token Generation Probability (TGP) and perplexity were used to evaluate the model’s behavior on privacy-sensitive data.</p></li>
<li><p>Correlation analysis between training data frequency and generation likelihood provided insights into the relationship between data occurrence and privacy risks.</p></li>
</ul>
</li>
</ol>
</section>
</section>
</section>
<section id="results">
<h1>Results<a class="headerlink" href="#results" title="Link to this heading">#</a></h1>
<p><img alt="image info" src="_images/pii_count_by_type.png" /></p>
<p><strong>Figure 1</strong>: <em>PII Count by Type</em>. This bar chart displays the total count of various types of Personally Identifiable Information (PII) detected in the dataset. The analysis highlights “PERSON” entities as the most frequent, followed by “GPE” (Geopolitical Entities), while phone numbers and email addresses appear less frequently.</p>
<p><img alt="image info" src="_images/pii_rate_per_document.png" /></p>
<p><strong>Figure 2</strong>: <em>PII Rate per Document</em>. This plot shows the average rate of occurrence of each PII type per document in the dataset. “PERSON” and “GPE” entities are the most prevalent, occurring in over 50% of the documents, indicating significant potential privacy risks.</p>
<p><img alt="image info" src="_images/top_entities_person.png" /></p>
<p><strong>Figure 3</strong>: <em>Top Entities: PERSON</em>. This visualization lists the top 50 most frequent “PERSON” entities detected in the dataset. Names such as “Obama” and “Jesus” dominate the list, highlighting the potential for memorization of high-frequency names.</p>
<p><img alt="image info" src="_images/top_entities_email.png" /></p>
<p><strong>Figure 4</strong>: <em>Top Entities: EMAIL</em>. This chart illustrates the most frequent email addresses identified in the dataset. While some addresses appear innocuous, their frequent occurrence underscores the risk of privacy leaks involving sensitive communications.</p>
<p><img alt="image info" src="_images/top_entities_ip_address.png" /></p>
<p><strong>Figure 5</strong>: <em>Top Entities: IP Address</em>. The most common IP addresses detected in the dataset are shown here. Both public and private IP addresses appear, with certain private addresses posing a higher risk of unintended exposure if memorized by the model.</p>
<p><img alt="image info" src="_images/analysis_regression.png" /></p>
<p><strong>Figure 6</strong>: <em>Regression Analysis on Data Occurrence and Generation Probability</em>. This scatter plot with a fitted regression line shows the relationship between the frequency of sensitive data in the training dataset and the likelihood of it being generated by the model. The positive slope indicates a statistically significant correlation, with higher frequency data being more likely to reappear in the outputs. Statistical details, such as the ( R^2 ) value and p-value, support the strength and significance of the relationship.</p>
</section>
<section id="analysis">
<h1>Analysis<a class="headerlink" href="#analysis" title="Link to this heading">#</a></h1>
<section id="data-privacy-leakage-distribution">
<h2>Data Privacy Leakage Distribution<a class="headerlink" href="#data-privacy-leakage-distribution" title="Link to this heading">#</a></h2>
<section id="pii-type-distribution">
<h3>1. PII Type Distribution<a class="headerlink" href="#pii-type-distribution" title="Link to this heading">#</a></h3>
<p>The bar chart in Figure 2 shows the total count of various PII (Personally Identifiable Information) types detected across the dataset. The data indicates that “PERSON” entities are the most frequent, followed by “GPE” (Geopolitical Entities). Other PII types such as phone numbers and email addresses have significantly lower frequencies. This distribution emphasizes the potential risk areas for privacy leakage, particularly in the context of models trained on extensive text sources that may contain personal or organizational data.</p>
</section>
<section id="pii-occurrence-rate-per-document">
<h3>2. PII Occurrence Rate per Document<a class="headerlink" href="#pii-occurrence-rate-per-document" title="Link to this heading">#</a></h3>
<p>Figure 3 presents the rate of each PII type’s occurrence per document, showcasing how prevalent each type of sensitive information is within individual documents. The analysis points out that both “PERSON” and “GPE” entities are present in over 50% of documents, making them the most common types of sensitive information. This high occurrence rate poses a notable risk for privacy exposure in model outputs.</p>
</section>
<section id="detailed-analysis-of-person-entities">
<h3>3. Detailed Analysis of “PERSON” Entities<a class="headerlink" href="#detailed-analysis-of-person-entities" title="Link to this heading">#</a></h3>
<p>Figure 4 highlights the top 50 most frequent “PERSON” entities in the dataset, with “Obama” appearing most frequently, followed by “Jesus” and other common names. This breakdown underscores the memorization risk associated with high-frequency names, which are more likely to be generated by LLMs when prompted, potentially exposing sensitive or context-specific information.</p>
</section>
<section id="analysis-of-ip-address-entities">
<h3>4. Analysis of “IP_ADDRESS” Entities<a class="headerlink" href="#analysis-of-ip-address-entities" title="Link to this heading">#</a></h3>
<p>Figure 5 showcases the top 50 most frequent “IP_ADDRESS” entities detected in the dataset. The results indicate a range of public and private IP addresses, with some appearing more than 15 times. Although the presence of public or commonly used IP addresses may not always pose privacy risks, the repeated appearance of certain private IP addresses or those associated with internal networks highlights a potential source of unintended privacy exposure.</p>
</section>
</section>
<section id="effect-of-data-privacy-leakage-on-llms">
<h2>Effect of Data Privacy Leakage on LLMs<a class="headerlink" href="#effect-of-data-privacy-leakage-on-llms" title="Link to this heading">#</a></h2>
<section id="regression-analysis-and-statistical-significance">
<h3>1.Regression Analysis and Statistical Significance<a class="headerlink" href="#regression-analysis-and-statistical-significance" title="Link to this heading">#</a></h3>
<p>The linear regression analysis provides further insights into the relationship between occurrences and generation probabilities. The fitted regression line has a <strong>slope</strong> of approximately 0.087, indicating a positive but modest increase in generation probability as occurrences rise. The <strong>intercept</strong> of approximately 0.4163 suggests that when occurrences approach zero, the baseline generation probability is slightly negative, meaning if the sample never occurs in training data, it won’t be generated.</p>
<p>The <strong>p-value</strong> for the slope, approximately <span class="math notranslate nohighlight">\(9.6 \times 10^{-3}\)</span>, indicates strong statistical significance at conventional thresholds (e.g., 0.05). This result supports the hypothesis that there is a meaningful linear relationship between occurrences in training data and generation probability. The <strong>standard error</strong> of approximately 0.0000844 provides an estimate of the precision of the slope, suggesting that while there is variability, the estimate is reliable.</p>
</section>
<section id="implications-of-the-findings">
<h3>2. Implications of the Findings<a class="headerlink" href="#implications-of-the-findings" title="Link to this heading">#</a></h3>
<p>These results emphasize that while there is a statistically significant positive correlation between data occurrences and generation probabilities, the strength of this correlation is moderate. This aligns with the observed trend where more frequently occurring data points in the training set tend to appear more often during inference.  In simple terms, the more a piece of sensitive information is repeated in the training data, the more likely the model is to remember and accidentally reveal it.</p>
<p>Understanding these dynamics can be crucial for evaluating the robustness and privacy implications of LLMs. For instance, an elevated generation probability for rare training examples could expose memorized content, raising concerns about the model’s handling of sensitive or proprietary information. This confirms that data leakage can trickle down to LLM behavior, where even rare or isolated data points, if memorized, can surface during inference, posing privacy risks. As highlighted in the analysis, the presence of outliers in the data suggests that LLMs might retain specific details that extend beyond their intended training distribution, leading to potential inadvertent disclosures. This underscores the importance of scrutinizing LLMs not just for overall performance but also for vulnerabilities in handling sensitive or infrequent data.</p>
</section>
</section>
</section>
<section id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h1>
<p>This study underscores the critical need for privacy-preserving strategies in large language models (LLMs), revealing that sensitive information exists in the training data of our AI chatbot and can impact generation, making privacy-sensitive content more likely to appear. Specific entities, such as “PERSON” and “GPE,” are particularly prone to reoccurrence in outputs due to their frequency in training data, while even unique data points, like names and IP addresses, may be memorized and generated. Addressing these risks is essential for regulatory compliance, user trust, and sustainable AI development. Privacy-preserving techniques, such as differential privacy and data anonymization, can mitigate these risks, and comprehensive regulatory frameworks will be crucial to balancing privacy, data utility, and innovation, fostering responsible AI. This study provides foundational insights into privacy leaks in LLMs and emphasizes the importance of ongoing research to ensure safe and ethical AI deployment.</p>
</section>
<section id="bibliography">
<h1>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h1>
<div class="docutils container" id="id16">
<div role="list" class="citation-list">
<div class="citation" id="id22" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>. 2017.</p>
</div>
<div class="citation" id="id23" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">2</a><span class="fn-bracket">]</span></span>
<p>A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 2019.</p>
</div>
<div class="citation" id="id24" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">3</a><span class="fn-bracket">]</span></span>
<p>C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. <em>Journal of Machine Learning Research</em>, 2020.</p>
</div>
<div class="citation" id="id25" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">4</a><span class="fn-bracket">]</span></span>
<p>T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>. 2020.</p>
</div>
<div class="citation" id="id26" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">5</a><span class="fn-bracket">]</span></span>
<p>J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em>. 2019.</p>
</div>
<div class="citation" id="id27" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">6</a><span class="fn-bracket">]</span></span>
<p>A. Esteva, A. Robicquet, B. Ramsundar, V. Kuleshov, M. DePristo, K. Chou, C. Cui, G. Corrado, S. Thrun, and J. Dean. A guide to deep learning in healthcare. <em>Nature Medicine</em>, 2019.</p>
</div>
<div class="citation" id="id28" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">7</a><span class="fn-bracket">]</span></span>
<p>M. J. Bommarito and D. M. Katz. A study of artificial intelligence in legal document analysis. <em>Journal of Artificial Intelligence Research</em>, 2018.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">8</a><span class="fn-bracket">]</span></span>
<p>N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, A. Oprea, and C. Raffel. Extracting training data from large language models. In <em>USENIX Security Symposium</em>. 2021.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">9</a><span class="fn-bracket">]</span></span>
<p>R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In <em>Proceedings of the 2017 IEEE Symposium on Security and Privacy</em>. 2017.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">10</a><span class="fn-bracket">]</span></span>
<p>B. Jayaraman and D. Evans. Evaluating differentially private machine learning in practice. In <em>Proceedings of the 28th USENIX Security Symposium</em>. 2019.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">11</a><span class="fn-bracket">]</span></span>
<p>M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>. 2016.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">12</a><span class="fn-bracket">]</span></span>
<p>M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfinkel, A. Dafoe, P. Scharre, T. Zeitzoff, B. Filar, H. Anderson, H. Roff, R. Crootof, O. Evans, M. Page, J. Bryson, R. Yampolskiy, and D. Amodei. The malicious use of artificial intelligence: forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228, 2018.</p>
</div>
<div class="citation" id="id17" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">13</a><span class="fn-bracket">]</span></span>
<p>S. Ruggles. When privacy protection goes wrong: how and why the 2020 census confidentiality program failed. <em>Journal of Economic Perspectives</em>, 2024. URL: <a class="reference external" href="https://doi.org/10.1257/JEP.38.2.201">https://doi.org/10.1257/JEP.38.2.201</a>, <a class="reference external" href="https://doi.org/10.1257/JEP.38.2.201">doi:10.1257/JEP.38.2.201</a>.</p>
</div>
<div class="citation" id="id18" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">14</a><span class="fn-bracket">]</span></span>
<p>A. R. Santos-Lozada, J. T. Howard, and A. M. Verdery. How differential privacy will affect our understanding of health disparities in the united states. <em>Proceedings of the National Academy of Sciences</em>, 2020. URL: <a class="reference external" href="https://doi.org/10.1073/PNAS.2003714117">https://doi.org/10.1073/PNAS.2003714117</a>, <a class="reference external" href="https://doi.org/10.1073/PNAS.2003714117">doi:10.1073/PNAS.2003714117</a>.</p>
</div>
<div class="citation" id="id19" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">15</a><span class="fn-bracket">]</span></span>
<p>K. Muralidhar and J. Domingo-Ferrer. A rejoinder to garfinkel (2023) – legacy statistical disclosure limitation techniques for protecting 2020 decennial us census: still a viable option. <em>Journal of Official Statistics</em>, 2023. URL: <a class="reference external" href="https://doi.org/10.2478/JOS-2023-0019">https://doi.org/10.2478/JOS-2023-0019</a>, <a class="reference external" href="https://doi.org/10.2478/JOS-2023-0019">doi:10.2478/JOS-2023-0019</a>.</p>
</div>
<div class="citation" id="id20" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">16</a><span class="fn-bracket">]</span></span>
<p>V. J. Hotz, C. Bollinger, T. Komarova, C. Manski, R. Moffitt, D. Nekipelov, A. J. Sojourner, and B. Spencer. Balancing data privacy and usability in the federal statistical system. <em>Proceedings of the National Academy of Sciences</em>, 2022. URL: <a class="reference external" href="https://doi.org/10.1073/PNAS.2104906119">https://doi.org/10.1073/PNAS.2104906119</a>, <a class="reference external" href="https://doi.org/10.1073/PNAS.2104906119">doi:10.1073/PNAS.2104906119</a>.</p>
</div>
<div class="citation" id="id21" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">17</a><span class="fn-bracket">]</span></span>
<p>J. Domingo-Ferrer, D. Sánchez, and A. Blanco-Justicia. The limits of differential privacy (and its misuse in data release and machine learning). <em>Communications of the ACM</em>, 2021. URL: <a class="reference external" href="https://doi.org/10.1145/3433638">https://doi.org/10.1145/3433638</a>, <a class="reference external" href="https://doi.org/10.1145/3433638">doi:10.1145/3433638</a>.</p>
</div>
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">18</a><span class="fn-bracket">]</span></span>
<p>Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: decanting the web for the finest text data at scale. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2406.17557">https://arxiv.org/abs/2406.17557</a>, <a class="reference external" href="https://arxiv.org/abs/2406.17557">arXiv:2406.17557</a>.</p>
</div>
</div>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Abstract</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#privacy-leakage-in-llms">Privacy Leakage in LLMs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#method">Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analytical-framework">Analytical Framework</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experimental-setup">Experimental Setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-preparation">Dataset Preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">Model Evaluation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis">Analysis</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-privacy-leakage-distribution">Data Privacy Leakage Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pii-type-distribution">1. PII Type Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pii-occurrence-rate-per-document">2. PII Occurrence Rate per Document</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-analysis-of-person-entities">3. Detailed Analysis of “PERSON” Entities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-ip-address-entities">4. Analysis of “IP_ADDRESS” Entities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-data-privacy-leakage-on-llms">Effect of Data Privacy Leakage on LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis-and-statistical-significance">1.Regression Analysis and Statistical Significance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implications-of-the-findings">2. Implications of the Findings</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">Bibliography</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Terry Taiming Lu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>